<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Papers</title>

    <meta name="author" content="Junliang He">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/jpg" href="images/fudan_icon.jpg">

    <script type="text/javascript">

        function display(id) {
            var traget = document.getElementById(id);
            if (traget.style.display == "none") {
                traget.style.display = "";
            } else {
                traget.style.display = "none";
            }
        }  
    </script>
</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <name>Papers</name>
                    <p>
                        (*: Equal contribution)
                    </p>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Preprint</heading>
                                    
                                    <ul>
                                        <li>
                                            <a href="https://arxiv.org/abs/2210.07626">
                                                <papertitle>BERTScore is Unfair: On Social Bias in Language Model-Based
                                                    Metrics for Text Generation</papertitle>
                                            </a>
                                            <br>
                                            <strong>Tianxiang Sun</strong>*, Junliang He*, Xipeng Qiu, Xuanjing Huang
                                            <br>
                                            <em>EMNLP</em> 2022
                                            <br>
                                            <a href="https://arxiv.org/pdf/2210.07626.pdf">pdf</a>
                                            /
                                            <a onclick="return display('abs_metric_bias');">abstract</a>
                                            /
                                            <a href="https://github.com/txsun1997/Metric-Fairness">code</a>
                                            <p></p>
                                            <div id="abs_metric_bias" style="display: none;">
                                                Automatic evaluation metrics are crucial to the development of
                                                generative systems.
                                                In recent years, pre-trained language model (PLM) based metrics, such as
                                                BERTScore, have been commonly adopted in
                                                various generation tasks.
                                                However, it has been demonstrated that PLMs encode a range of
                                                stereotypical societal biases, leading to a concern on the fairness of
                                                PLMs as metrics.
                                                To that end, this work presents the first systematic study on the social
                                                bias in PLM-based metrics.
                                                We demonstrate that popular PLM-based metrics exhibit significantly
                                                higher social bias than traditional metrics on 6 sensitive attributes,
                                                namely race, gender, religion, physical appearance, age, and
                                                socioeconomic status.
                                                In-depth analysis suggests that choosing paradigms (matching,
                                                regression, or generation) of the metric has a greater impact on
                                                fairness than choosing PLMs.
                                                In addition, we develop debiasing adapters that are injected into PLM
                                                layers, mitigating bias in PLM-based metrics while retaining high
                                                performance for evaluating text generation.
                                            </div>
                                        </li>
                                    </ul>
                                    
                                    <ul>
                                        <li>
                                            <a href="https://aclanthology.org/2022.naacl-main.240/">
                                                <papertitle>Towards Efficient NLP: A Standard Evaluation and A Strong
                                                    Baseline</papertitle>
                                            </a>
                                            <br>
                                            Xiangyang Liu*, <strong>Tianxiang Sun</strong>*, Junliang He, Jiawen Wu,
                                            Lingling Wu, Xinyu Zhang, Hao Jiang, Zhao Cao, Xuanjing Huang, Xipeng Qiu
                                            <br>
                                            <em>NAACL</em> 2022 &nbsp <font color="red"><strong>(Oral
                                                    Presentation)</strong></font>
                                            <br>
                                            <a href="https://aclanthology.org/2022.naacl-main.240.pdf">pdf</a>
                                            /
                                            <a onclick="return display('abs_elue');">abstract</a>
                                            /
                                            <a href="https://github.com/fastnlp/ElasticBERT">code</a>
                                            /
                                            <a href="http://eluebenchmark.fastnlp.top/">benchmark</a>
                                            /
                                            <a href="https://txsun1997.github.io/slides/ELUE.pdf">slides</a>
                                            <p></p>
                                            <div id="abs_elue" style="display: none;">
                                                Supersized pre-trained language models have pushed the accuracy of
                                                various natural language processing (NLP) tasks to a new
                                                state-of-the-art (SOTA). Rather than pursuing the reachless SOTA
                                                accuracy, more and more researchers start paying attention to model
                                                efficiency and usability. Different from accuracy, the metric for
                                                efficiency varies across different studies, making them hard to be
                                                fairly compared. To that end, this work presents ELUE (Efficient
                                                Language Understanding Evaluation), a standard evaluation, and a public
                                                leaderboard for efficient NLP models. ELUE is dedicated to depicting the
                                                Pareto Frontier for various language understanding tasks, such that it
                                                can tell whether and how much a method achieves Pareto improvement.
                                                Along with the benchmark, we also release a strong baseline,
                                                ElasticBERT, which allows BERT to exit at any layer in both static and
                                                dynamic ways. We demonstrate the ElasticBERT, despite its simplicity,
                                                outperforms or performs on par with SOTA compressed and early exiting
                                                models. With ElasticBERT, the proposed ELUE has a strong Pareto Frontier
                                                and makes a better evaluation for efficient NLP models.
                                            </div>
                                        </li>
                                    </ul>
                                    
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </td>
            </tr>
    </table>
</body>